{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Formative assessment\n",
    "### Week 7: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, you will write code to implement and train a Transformer classifier model.\n",
    "\n",
    "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "These cells require you to write your own code to complete them.\n",
    "\n",
    "#### Let's get started!\n",
    "\n",
    "We'll start by running some imports, and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell to import all required packages. \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (TextVectorization, Dense, MultiHeadAttention, LayerNormalization, \n",
    "                                     Layer, Embedding, Dropout, GlobalAveragePooling1D)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/IMDb.png\" title=\"IMDb\" style=\"width: 550px;\"/>  \n",
    "  \n",
    "#### The IMDb dataset\n",
    "\n",
    "In this assignment, you will use the [IMDb dataset](https://https://www.imdb.com/interfaces/). This is a sentiment analysis dataset of movie reviews with binary labels. It contains 25,000 training examples and a further 25,000 for testing. \n",
    "\n",
    "* Maas, A.L.,  Daly, R.E.,  Pham, P.T.,  Huang, D.,  Ng, A.Y. & Potts, C. (2011), \"Learning Word Vectors for Sentiment Analysis\", _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, 142-150.\n",
    "\n",
    "Your goal is to build and train an encoder-only Transformer classifier model to predict the sentiment labels from the review text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare the data\n",
    "For this assignment, you will load the IMDb dataset from the TensorFlow Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 22:47:05.514831: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/xinyuhu/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c040d5eddf9e4f1386dd40b3b457b12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288a03cd34124a3b81925cf58f38bdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d9d9513fe94eb59fbe3290cc8693ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6464c2fd90846618492c3e9601d53ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f75614d2734c1ba5afaaedec6e680f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/xinyuhu/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteUM50O8/imdb_reviews-train…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0ad4da614d4e1ca5b11b3257f21ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e1f2f5274840e29b8fce28d55e1694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/xinyuhu/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteUM50O8/imdb_reviews-test.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c68826b76434c29a30e842eb58c10e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2523aa817c04247abd59ee997cf98c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/xinyuhu/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteUM50O8/imdb_reviews-unsup…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/xinyuhu/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'text': TensorSpec(shape=(), dtype=tf.string, name=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to load the data and print the element_spec\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_data = tfds.load(\"imdb_reviews\", split=\"train\", read_config=tfds.ReadConfig(try_autocache=False))\n",
    "test_data = tfds.load(\"imdb_reviews\", split=\"test\", read_config=tfds.ReadConfig(try_autocache=False))\n",
    "\n",
    "train_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a film which should be seen by anybody interested in, effected by, or suffering from an eating disorder. It is an amazingly accurate and sensitive portrayal of bulimia in a teenage girl, its causes and its symptoms. The girl is played by one of the most brilliant young actresses working in cinema today, Alison Lohman, who was later so spectacular in 'Where the Truth Lies'. I would recommend that this film be shown in all schools, as you will never see a better on this subject. Alison Lohman is absolutely outstanding, and one marvels at her ability to convey the anguish of a girl suffering from this compulsive disorder. If barometers tell us the air pressure, Alison Lohman tells us the emotional pressure with the same degree of accuracy. Her emotional range is so precise, each scene could be measured microscopically for its gradations of trauma, on a scale of rising hysteria and desperation which reaches unbearable intensity. Mare Winningham is the perfect choice to play her mother, and does so with immense sympathy and a range of emotions just as finely tuned as Lohman's. Together, they make a pair of sensitive emotional oscillators vibrating in resonance with one another. This film is really an astonishing achievement, and director Katt Shea should be proud of it. The only reason for not seeing it is if you are not interested in people. But even if you like nature films best, this is after all animal behaviour at the sharp edge. Bulimia is an extreme version of how a tormented soul can destroy her own body in a frenzy of despair. And if we don't sympathise with people suffering from the depths of despair, then we are dead inside.\n",
      "Label: 1\n",
      "\n",
      "One of the weaker Carry On adventures sees Sid James as the head of a crime gang stealing contraceptive pills. The fourth of the series to be hospital-based, it's possibly the least of the genre. There's a curiously flat feel throughout, with all seemingly squandered on below-par material. This is far from the late-70s nadir, but Williams, James, Bresslaw, Maynard et al. are all class performers yet not given the backing of a script equal to their ability.<br /><br />Most of the gags are onrunning, rather than episodic as Carry Ons usually are. So that instead of the traditional hit and miss ratio, if you don't find the joke funny in the first place you're stuck with it for most of the film. These continuous plot strands include Williams  for no good reason  worrying that he's changing sex, and Kenneth Cope in drag. Like the stagy physical pratt falls, the whole thing feels more contrived than in other movies, and lacking in cast interest. Continuing this theme, Matron lacks the customary pun and innuendo format, largely opting for characterisation and consequence to provide the humour. In fact, the somewhat puerile series of laboured misunderstandings and forced circumstance reminds one more of Terry and June ... so it's appropriate that Terry Scott is present, mugging futilely throughout.<br /><br />Some dialogue exchanges have a bit of the old magic, such as this between Scott and Cope: \"What about a little drink?\" \"Oh, no, no, I never touch it.\" \"Oh. Cigarette then?\" \"No, I never touch them.\" \"That leaves only one thing to offer you.\" \"I never touch that either.\" That said, while a funny man in his own right (livening up the duller episodes of Randall and Hopkirk (Deceased) no end), you do feel that Cope isn't quite tapped in to the self-parodying Carry On idealology and that Bernard Bresslaw dressed as a nurse would be far funnier. This does actually happen, in part, though only for the last fifteen minutes.<br /><br />Williams attempting to seduce Hattie Jacques while Charles Hawtrey is hiding in a cupboard is pure drawer room farce, but lacks the irony to carry it off. That said, Williams's description of premarital relations is priceless: \"You don't just go into the shop and buy enough for the whole room, you tear yourself off a little strip and try it first!\" \"That may be so,\" counters Jacques, \"but you're not going to stick me up against a wall.\" Williams really comes to life in his scenes with Hattie, and you can never get bored of hearing a tin whistle whenever someone accidentally flashes their knickers.<br /><br />Carry On Matron is not a bad film by any means, just a crushingly bog-standard one.\n",
      "Label: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View some samples\n",
    "\n",
    "for example in train_data.shuffle(100).take(2):\n",
    "    print(example['text'].numpy().decode(\"utf-8\"))\n",
    "    print(f\"Label: {example['label'].numpy()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the input sentences\n",
    "\n",
    "We will need to convert the text into integer tokens to be able to process them in the Transformer. To do this we will use a `TextVectorization` layer and adapt it to the training data. You should now complete the following function to create and prepare this layer as follows:\n",
    "\n",
    "* The function takes a `dataset` (a `tf.data.Dataset` object) as an argument, which has the same spec as `train_data` or `test_data` above. It also takes a `max_tokens` argument\n",
    "* The `TextVectorization` should be configured to use a maximum of `max_tokens` tokens (including the masking and OOV tokens)\n",
    "* It should standardize the input text by lower-casing the text and removing punctuation\n",
    "* It should split the text on whitespace\n",
    "* You should use the `adapt` method to compute the vocabulary using `dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def configure_textvectorization(dataset, max_tokens):\n",
    "    \"\"\"\n",
    "    This function should create a TextVectorization layer and configure it as above.\n",
    "    The function should then return the TextVectorization layer.\n",
    "    \"\"\"\n",
    "    textvectorization = TextVectorization(max_tokens=max_tokens, \n",
    "                                          standardize='lower_and_strip_punctuation',  # default\n",
    "                                          split='whitespace'  # default\n",
    "                                         )\n",
    "    textvectorization.adapt(dataset.map(lambda x: x['text']))  # Can also batch the dataset\n",
    "    return textvectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to create and configure the TextVectorization layer\n",
    "\n",
    "MAX_TOKENS = 20000\n",
    "text_vectorization = configure_textvectorization(train_data, max_tokens=MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[    2 13077   171    14   946     6     1    21     2  1045  1245     5\n",
      "     2  8454  3374     5     2   167   398   131     2  5611     1   754\n",
      "  3121     8    17 19606     1 14928     5     2   262   186   450     2\n",
      "     1  2623   177     3    47    50   225   981     2    18 10402   354\n",
      "    16    47  5578   233    30     4     1   361    32     2    97   697\n",
      "   875     6 12123     2   330   361  1200     7    47  1521   260    36\n",
      "  3587   172  2876     1     4   164  7176  1615   640    16   454    59\n",
      "    26  1618 14219     2  8897     5  4736  1159   708     4  2963     1\n",
      "    39  1982  4816   227    26   224   140    17     2     1   818    19\n",
      "     1  4326     9    15    41   154     1  3416   100    84    12  1588\n",
      "     1   764  6299   430   145   161   132   576   607    15  1454  1596\n",
      "     3 12944    31     2   167    62  1230   811  9189  3131  1810   261\n",
      "   126     2   413    15     4     1  1900   668     1     7    57   152\n",
      "   172    19   825   183     6   545     9     6    12 18274   111   591\n",
      "  5402  6667    37  4326    40   214    17  7834 11689     3  8667   100\n",
      "    84    55   268     8  2867    56  1978  1734     9   893   320    78\n",
      "   642     1    37    82   158   329    19  3735   143     1     8   764\n",
      "     1  2203  1620], shape=(207,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[    2   290  4456    44   203    75    47     5     2   106   152    12\n",
      "    10    26   434    10   116    41    43   168    29     5     2  3075\n",
      "    12    35    26    91    10   116    32  1535     5     2  4456  5927\n",
      " 13248  6365  2751   885     3  5927   885    32     5     2  3075    24\n",
      "   650     3    79   333   106    81    85   152     3  1589    60     4\n",
      "   169     5    98    14     8   106     5     2  3075     8    56   656\n",
      "     2   290  4456     7    47     5     2   797   152   122     3     7\n",
      "     2    32    62  1461   220   774    13    13    29     5    56   499\n",
      "  4456  3075    17 13248     7   589    81    71  3033  6083    32  3308\n",
      "     8    11   348    24     2   300  6079 16099  5172  1424  9010     1\n",
      "     1     1  1738     1  2337     1     1  4264     1  6389     3 11033\n",
      "     1  1424     3 16099  1679    85   347   132    48    24    38   106\n",
      "   162   510   132    11     7     4    53   650   348    48     7   154\n",
      "   712   290  4456   348    39    11    29   483     1 16210     3    10\n",
      "   368   191], shape=(170,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Test your TextVectorization layer\n",
    "\n",
    "for example in train_data.shuffle(100).take(2):\n",
    "    print(text_vectorization(example['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the datasets\n",
    "\n",
    "You should now complete the following function `preprocess_dataset` which you will use to preprocess the `train_data` and `test_data` Dataset objects according to the following spec:\n",
    "\n",
    "* The function takes `dataset`, `text_vectorization_layer`, `max_seq_len`, `batch_size` and `shuffle_buffer_size` as arguments\n",
    "    * `dataset` is a `tf.data.Dataset` object with the same spec as `train_data` or `test_data` above\n",
    "* The `text_vectorization_layer` should be used to convert the text into integer tokens\n",
    "* The maximum length of the token sequences should be `max_seq_len`. Any token sequences longer than this should be truncated\n",
    "* The Datasets should return a tuple of `(tokens, label)` Tensors\n",
    "* The Datasets should be shuffled with buffer size `shuffle_buffer_size`, and then batched with `batch_size`. Note that the sequences will not be the same length, so the batches should be padded with zero masking tokens where necessary (see [the docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch))\n",
    "* The function should then return the preprocessed `dataset` Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def preprocess_dataset(dataset, text_vectorization_layer, max_seq_len, batch_size, shuffle_buffer_size):\n",
    "    \"\"\"\n",
    "    This function should preprocess the Dataset object as above.\n",
    "    The function should then return the preprocessed Dataset.\n",
    "    \"\"\"\n",
    "    def _inputs_and_targets(example):\n",
    "        return example['text'], example['label']\n",
    "    \n",
    "    def _integer_tokens(text, label):\n",
    "        return text_vectorization_layer(text), label\n",
    "    \n",
    "    def _truncate_seq(tokens, label):\n",
    "        return tokens[:max_seq_len], label\n",
    "    \n",
    "    dataset = dataset.map(_inputs_and_targets).map(_integer_tokens).map(_truncate_seq)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).padded_batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to preprocess the Datasets\n",
    "\n",
    "MAX_SEQ_LEN = 200\n",
    "train_data = preprocess_dataset(train_data, text_vectorization, MAX_SEQ_LEN, \n",
    "                                batch_size=32, shuffle_buffer_size=500)\n",
    "test_data = preprocess_dataset(test_data, text_vectorization, MAX_SEQ_LEN, \n",
    "                               batch_size=32, shuffle_buffer_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, None), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "train_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 200), dtype=int64, numpy=\n",
      "array([[   29,     5,    56, ...,     0,     0,     0],\n",
      "       [   11,     7,  1402, ...,  8526,    21,     4],\n",
      "       [   10,   378,   632, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 2724,   356,     3, ...,     0,     0,     0],\n",
      "       [   49,  3372,   984, ...,    32,     8,    32],\n",
      "       [    9,   110, 14022, ...,    42,  3459,  1533]])>, <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
      "array([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Inspect a data minibatch\n",
    "\n",
    "for example in train_data.take(1):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we pass this integer tokens Tensor through our Transformer, we will need to be careful to not use the zero padding tokens. The mechanism to handle this is masking (see [this guide](https://www.tensorflow.org/guide/keras/masking_and_padding)), and our custom layers will need to make use of this masking mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer architecture\n",
    "\n",
    "We will use an encoder-only Transformer classifier architecture for the task of sentiment prediction. This will consist of a single encoder block, followed by a classifier head.\n",
    "\n",
    "<img src=\"figures/encoder-only_transformer.png\" alt=\"Transformer\" style=\"width: 250px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional encodings and Embedding layer\n",
    "\n",
    "You will now implement the input embedding and positional encoding stage of the Transformer. Your model will use the deterministic positional encoding scheme $\\mathbf{P}\\in\\mathbb{R}^{n\\times d_{model}}$ as in the original Transformer:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_{ti} &= \\left\\{\n",
    "\\begin{array}{c}\n",
    "\\sin(\\omega_k t)\\quad\\text{for }i=2k+1,\\quad(\\text{for some }k\\in\\mathbb{N}_0)\\\\\n",
    "\\cos(\\omega_k t)\\quad\\text{for }i=2k+2\\quad(\\text{for some }k\\in\\mathbb{N}_0)\n",
    "\\end{array}\n",
    "\\right.\\\\\n",
    "\\omega_k &= \\frac{1}{10000^{2k/d_{model}}},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $t=1,\\ldots,n$ and $i=1,\\ldots,d_{model}$.\n",
    "\n",
    "You should now complete the following `positional_encodings` function to compute the positional encoding $\\mathbf{P}\\in\\mathbb{R}^{n\\times d_{model}}$.\n",
    "\n",
    "* The function takes `seq_len` (the number of time steps) and `d_model` the embedding dimension as integer arguments\n",
    "* The function should compute a 2D Tensor of shape `(seq_len, d_model)` of positional encodings according to the above equations (be careful with python's zero-indexing, the python indices are off-by-one from the mathematical indices above)\n",
    "* The function should then return the Tensor of positional encodings with type `tf.float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def positional_encodings(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    This function should compute the positional encodings as above.\n",
    "    The function should then return the Tensor of positional encodings.\n",
    "    \"\"\"\n",
    "    max_wavelength = 10000.\n",
    "\n",
    "    pos = np.arange(seq_len)\n",
    "    inx = np.arange(d_model)\n",
    "\n",
    "    I, P = np.meshgrid(inx, pos)\n",
    "    pe_even = np.sin(P / max_wavelength**(I/d_model))\n",
    "    pe_odd = np.cos(P / max_wavelength**(I/d_model))\n",
    "        \n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    pe[:, ::2] = pe_even[:, ::2]\n",
    "    pe[:, 1::2] = pe_odd[:, ::2]\n",
    "    return tf.constant(pe, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the positional encodings\n",
    "\n",
    "D_MODEL = 32\n",
    "pos_encodings = positional_encodings(MAX_SEQ_LEN, D_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positional encodings should be added to the token embeddings in the first stage of the Transformer.\n",
    "\n",
    "You should now complete the `__init__` and `call` methods for the following custom layer `InputEmbeddings`, which builds and returns a model that converts the integer token sequence into a sequence of embeddings, and then adds positional encodings.\n",
    "\n",
    "* The initialiser takes the following required arguments:\n",
    "    * `d_model`: the dimension of the embedding vectors\n",
    "    * `pos_encodings`: the Tensor of positional encodings of shape `(max_seq_len, d_model)`, as computed by the `positional_encodings` function\n",
    "    * `max_tokens`: The maximum number of integer tokens used in the input (including the masking and OOV tokens)\n",
    "* The custom layer should create an `Embedding` layer with the correct input and output dimensions, that is set to mask incoming zero tokens. This layer should be set as the class attribute `embedding`\n",
    "* The `call` method takes a Tensor of integer tokens of shape `(batch_size, n)` as input, where `n` is the maximum sequence length in the batch\n",
    "* The `call` method should use the `Embedding` lookup layer to convert the inputs to embedding vectors, and return the sum of the embedding vectors and positional encodings\n",
    "\n",
    "_NB: The custom layer also implements the `compute_mask` method, which has been completed for you. This is a method should be implemented whenever a layer should produce a mask (see [this guide](https://www.tensorflow.org/guide/keras/masking_and_padding) for more information). Note that this method references `self.embedding`. In this instance we want to pass on the same mask that is produced by the `Embedding` layer, so the `compute_mask` method simply calls the existing method from the `Embedding` layer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following class.\n",
    "# Make sure not to change the class or methods name or arguments.\n",
    "\n",
    "class InputEmbeddings(Layer):\n",
    "    \"\"\"\n",
    "    This custom layer should take a batch of integer tokens as input, \n",
    "    converts them into embeddings and adds the positional encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, pos_encodings, max_tokens, name='input_embeddings', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.pos_encodings = pos_encodings\n",
    "        self.embedding = Embedding(max_tokens, d_model, mask_zero=True)\n",
    "        \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return self.embedding.compute_mask(inputs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs is an integer Tensor of shape (batch_size, n), where n is \n",
    "        the maximum sequence length in this batch (n \\le max_seq_len)\n",
    "        \"\"\"       \n",
    "        n = tf.shape(inputs)[-1]\n",
    "        pos_encodings = self.pos_encodings[:n, :]\n",
    "        h = self.embedding(inputs)\n",
    "        return h + pos_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your custom layer\n",
    "\n",
    "input_embeddings = InputEmbeddings(D_MODEL, pos_encodings, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your custom layer on an input\n",
    "\n",
    "for tokens, _ in train_data.take(1):\n",
    "    h = input_embeddings(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 200), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that our custom layer is producing a mask\n",
    "\n",
    "h._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder block\n",
    "\n",
    "You will now implement the encoder block of the Transformer model. This block consists of a multi-head attention block with a residual connection followed by layer normalisation, and then a pointwise feedforward network with residual connection again followed by layer normalisation.\n",
    "\n",
    "The multi-head attention block will need to account for the masking corresponding to the zero padding tokens in the input. The way the `MultiHeadAttention` layer handles this is through the `attention_mask` argument when it is called. \n",
    "\n",
    "The incoming mask is a boolean Tensor with shape `(batch_size, seq_len)`, where `seq_len` is the length of the sequence of embedding vectors being input to the `MultiHeadAttention` layer. The multi-head attention is performing self-attention, so the shape of the mask required by the `attention_mask` will be `(batch_size, seq_len, seq_len)`.\n",
    "\n",
    "Before implementing the encoder block, you should complete the following function `get_attention_mask`, which takes a single argument `mask`, which is a boolean Tensor of shape `(batch_size, seq_len)`, or `None`. If `mask` is `None`, then this function should return `None`. Otherwise, the function should return a boolean Tensor of shape `(batch_size, seq_len, seq_len)` which will be used by the `MultiHeadAttention` layer.\n",
    "\n",
    "For a single example in the batch, suppose the vector mask $\\mathbf{m}\\in\\mathbb{R}^n$, where $n$ is the sequence length. We would like to convert this vector mask to a matrix mask $\\mathbf{M}\\in\\mathbb{R}^{n\\times n}$, where the $i,j$-th element $M_{ij}$ is given by the element $\\min (i,j)$ of the vector $\\mathbf{m}$.\n",
    "\n",
    "For example, if the incoming boolean mask was as follows:\n",
    "\n",
    "```\n",
    "mask = [[True, True, False],\n",
    "        [True, False, False]]\n",
    "```\n",
    "\n",
    "where the batch size is 2 and the sequence length is 3, then the mask returned by the function `get_attention_mask` should look like:\n",
    "\n",
    "```\n",
    "returned_mask = [[[True, True, False],\n",
    "                  [True, True, False],\n",
    "                  [False, False, False]],\n",
    "                 [True, False, False],\n",
    "                 [False, False, False],\n",
    "                 [False, False, False]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_attention_mask(mask=None):\n",
    "    \"\"\"\n",
    "    This function should compute the attention mask as described above.\n",
    "    The function should then return the boolean Tensor.\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        return None\n",
    "    mask1 = mask[:, :, None]\n",
    "    mask2 = mask[:, None, :]\n",
    "    return mask1 & mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 3), dtype=bool, numpy=\n",
       "array([[[ True,  True, False],\n",
       "        [ True,  True, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[ True, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your mask function\n",
    "\n",
    "input_mask = tf.constant([[True, True, False], [True, False, False]])\n",
    "get_attention_mask(input_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should complete the following `EncoderBlock` custom layer, that implements the encoder block. \n",
    "\n",
    "* The initialiser takes the following required arguments:\n",
    "    * `num_heads`: the number of attention heads to use in the multi-head attention\n",
    "    * `key_dim`: the key (and query and value) dimension to use in the multi-head attention\n",
    "    * `d_model`: the embedding dimension of the Transformer\n",
    "    * `ff_dim`: the width of the hidden layer in the feedforward network in the encoder block\n",
    "* The initialiser sets `self.support_masking = True` (this has been done for you), so that the incoming boolean mask will also be included in the output of the `EncoderBlock` layer\n",
    "* The `EncoderBlock` layer should create `MultiHeadAttention`, `LayerNormalization` and `Dense` layers in the initializer as required \n",
    "    * The feedforward network should have one hidden layer of size `ff_dim` with a ReLU activation\n",
    "    * The output layer of the feedforward network should have size `d_model` and no activation\n",
    "* The operation of the custom layer is as follows:\n",
    "    * The input to the layer is a Tensor of shape `(batch_size, seq_len, d_model)`\n",
    "    * The call method also takes a `mask` argument, which will be a boolean mask of shape `(batch_size, seq_len)`. The call method should use your `get_attention_mask` function to compute the attention mask\n",
    "    * Pass the input through the `MultiHeadAttention` layer (performing self-attention), passing the attention mask in the `attention_mask`. Add the resulting Tensor output to the input (residual connection) and pass through a `LayerNormalization` layer\n",
    "    * Pass the resulting Tensor $h$ through the feedforward network, add this to $h$ (residual connection) and pass through a `LayerNormalization` layer\n",
    "    * The custom layer should then return the resulting Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following class.\n",
    "# Make sure not to change the class or methods name or arguments.\n",
    "\n",
    "class EncoderBlock(Layer):\n",
    "    \"\"\"\n",
    "    This custom layer should take a Tensor of shape (batch_size, seq_len, d_model) as input.\n",
    "    It should carry out the operations as described above and return the resulting Tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, key_dim, d_model, ff_dim, name='encoder_block', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.supports_masking = True  # This will pass on any incoming mask\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.d_model = d_model\n",
    "        self.ff_dim = ff_dim\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads, key_dim)\n",
    "        self.ff = Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization()\n",
    "        self.layernorm2 = LayerNormalization()\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        \"\"\"\n",
    "        inputs is a Tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"    \n",
    "        attention_mask = get_attention_mask(mask)\n",
    "        h = self.multihead_attention(inputs, inputs, attention_mask=attention_mask)\n",
    "        h = self.layernorm1(inputs + h)\n",
    "        \n",
    "        h_ff = self.ff(h)\n",
    "        return self.layernorm2(h + h_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EncoderBlock instance\n",
    "\n",
    "encoder_block = EncoderBlock(num_heads=2, key_dim=16, d_model=32, ff_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your layer on a dummy input\n",
    "\n",
    "inputs = tf.random.normal((16, 200, 32))\n",
    "h = encoder_block(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your layer on the output from the input_embeddings layer\n",
    "\n",
    "for tokens, _ in train_data.take(1):\n",
    "    h = input_embeddings(tokens)\n",
    "    h = encoder_block(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 200), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the mask has been propagated\n",
    "\n",
    "h._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier head\n",
    "\n",
    "The final stage of our Transformer model is the classifier head. This stage consists of the following layers:\n",
    "\n",
    "* A `GlobalAveragePooling1D` layer, that takes an incoming Tensor of shape `(batch_size, seq_len, d_model)` and reduces out the time axis to produce a Tensor of shape `(batch_size, d_model)`\n",
    "* A dropout layer\n",
    "* A dense layer with ReLU activation\n",
    "* A dropout layer\n",
    "* A dense layer with a single neuron output and sigmoid activation function\n",
    "\n",
    "The final dense layer outputs the probability of a positive sentiment label.\n",
    "\n",
    "You should now complete the following `get_classifier_head` function, which takes the arguments `d_model`, `dropout_rate` and `units`. The function should build and return a Sequential Model object, according to the above specification, where `dropout_rate` is used in both `Dropout` layers and `units` is used to define the width of the intermediate `Dense` layer. The `d_model` input should be used to set the `input_shape` in the first layer.\n",
    "\n",
    "Note that the [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) layer will automatically use the incoming mask when used in the `Sequential` model, see [the guide](https://www.tensorflow.org/guide/keras/masking_and_padding) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_classifier_head(d_model, dropout_rate, units):\n",
    "    \"\"\"\n",
    "    This function should compute classifier head model as described above.\n",
    "    The function should then return the Model object.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        GlobalAveragePooling1D(input_shape=(None, d_model)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the classifier head\n",
    "\n",
    "classifier_head = get_classifier_head(D_MODEL, dropout_rate=0.1, units=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 681 (2.66 KB)\n",
      "Trainable params: 681 (2.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "\n",
    "classifier_head.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 1), dtype=float32, numpy=\n",
       "array([[0.48969847],\n",
       "       [0.4935625 ],\n",
       "       [0.5036215 ],\n",
       "       [0.49018058],\n",
       "       [0.50510937],\n",
       "       [0.5138685 ],\n",
       "       [0.49028182],\n",
       "       [0.5169365 ]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your classifier head model on a dummy input\n",
    "\n",
    "inputs = tf.random.normal((8, 200, D_MODEL))\n",
    "classifier_head(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Transformer classifier\n",
    "\n",
    "We now have all the components to build the complete Transformer classifier. You should now complete the following function `get_transformer_classifier` to build and compile the model.\n",
    "\n",
    "The function takes the arguments `input_embeddings_layer`, `encoder_block_layer` and `classifier_head_layer`. It should use these layers to build a `Sequential` model, and compile it with a suitable loss function and optimizer, and an accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function.\n",
    "# Make sure not to change the function name or arguments.\n",
    "\n",
    "def get_transformer_classifier(input_embeddings_layer, encoder_block_layer, classifier_head_layer):\n",
    "    \"\"\"\n",
    "    This function should compute classifier head model as described above.\n",
    "    The function should then return the Model object.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        input_embeddings_layer,\n",
    "        encoder_block_layer,\n",
    "        classifier_head_layer\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the compiled Transformer classifier model\n",
    "\n",
    "input_embeddings = InputEmbeddings(D_MODEL, pos_encodings, MAX_TOKENS)\n",
    "encoder_block = EncoderBlock(num_heads=2, key_dim=16, d_model=32, ff_dim=32)\n",
    "classifier_head = get_classifier_head(D_MODEL, dropout_rate=0.1, units=20)\n",
    "transformer = get_transformer_classifier(input_embeddings, encoder_block, classifier_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Transformer classifier model\n",
    "\n",
    "for tokens, _ in train_data.take(1):\n",
    "    outputs = transformer(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.5239 - accuracy: 0.6960 - val_loss: 0.3380 - val_accuracy: 0.8529\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.2566 - accuracy: 0.9003 - val_loss: 0.3607 - val_accuracy: 0.8544\n"
     ]
    }
   ],
   "source": [
    "history = transformer.fit(train_data, validation_data=test_data, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on unlabelled data\n",
    "\n",
    "The IMDB dataset also contains a split without labels. The following cell loads this dataset split and applies a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_data = tfds.load(\"imdb_reviews\", split=\"unsupervised\", \n",
    "                              read_config=tfds.ReadConfig(try_autocache=False))\n",
    "unsupervised_data = unsupervised_data.shuffle(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at some model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Alpha Video release seems to be fairly complete with the entire story intact (except for some splicy sections in what was probably a 16mm television print: The story does make sense in this version which has the entire explanation of why the criminals are on the ship in the first place and what the doctor's motivations are.<br /><br />It is mysterious that the film runs about 63 minutes when the main IMDb description has it released at 57 minutes. That's probably incorrect and doesn't represent the original theatrical release, but rather some random individual's timing from a DVD or VHS tape that wasn't complete in the first place.\n",
      "\n",
      "Transformer probability of positive label: 0.03219905495643616\n"
     ]
    }
   ],
   "source": [
    "for example in unsupervised_data.take(1):\n",
    "    print(example['text'].numpy().decode(\"utf-8\"))\n",
    "    tokens = text_vectorization(example['text'])\n",
    "    tokens = tokens[tf.newaxis, :MAX_SEQ_LEN]  # Add dummy batch dimension and truncate\n",
    "    prob = transformer(tokens).numpy().squeeze()\n",
    "    print(f\"\\nTransformer probability of positive label: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this week's assignment! You have now implemented and trained an encoder-only Transformer classifier model for the task of sentiment prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
